{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_array(gen_array, sim_array, cut_array):\n",
    "    \n",
    "    id_particle = ak.to_numpy(gen_array[\"event\"])\n",
    "    id_reco = ak.to_numpy(sim_array[\"event\"])\n",
    "    id_both = ak.to_numpy(cut_array[\"event\"])\n",
    "    print(\"Start masking arrays\")\n",
    "    # Mask out overlapping events\n",
    "    mask_particle_only = ~np.isin(id_particle, id_both)\n",
    "    mask_reco_only = ~np.isin(id_reco, id_both)\n",
    "\n",
    "    particle_only = gen_array[mask_particle_only]\n",
    "    reco_only = sim_array[mask_reco_only]\n",
    "    \n",
    "    particle_only = ak.with_field(particle_only, True, \"pass_particle\")\n",
    "    particle_only = ak.with_field(particle_only, False, \"pass_reco\")\n",
    "\n",
    "    reco_only = ak.with_field(reco_only, False, \"pass_particle\")\n",
    "    reco_only = ak.with_field(reco_only, True, \"pass_reco\")\n",
    "\n",
    "    cut_array = ak.with_field(cut_array, True, \"pass_particle\")\n",
    "    cut_array = ak.with_field(cut_array, True, \"pass_reco\")\n",
    "    \n",
    "    print(\"Done masking\") \n",
    "    \n",
    "    reco_field_order = [\n",
    "    'run', 'event', 'eventWeight',\n",
    "    'ptl1', 'truth_ptl1',\n",
    "    'etal1', 'truth_etal1',\n",
    "    'phil1', 'truth_phil1',\n",
    "    'ptb1', 'truth_ptb1', 'ptb2', 'truth_ptb2',\n",
    "    'ptb3', 'truth_ptb3', 'ptb4', 'truth_ptb4',\n",
    "    'etab1', 'truth_etab1', 'etab2', 'truth_etab2',\n",
    "    'etab3', 'truth_etab3', 'etab4', 'truth_etab4',\n",
    "    'phib1', 'truth_phib1', 'phib2', 'truth_phib2',\n",
    "    'phib3', 'truth_phib3', 'phib4', 'truth_phib4',\n",
    "    'mb1', 'truth_mb1', 'mb2', 'truth_mb2',\n",
    "    'mb3', 'truth_mb3', 'mb4', 'truth_mb4',\n",
    "    'ptj1', 'truth_ptj1', 'ptj2', 'truth_ptj2',\n",
    "    'ptj3', 'truth_ptj3', 'ptj4', 'truth_ptj4',\n",
    "    'ptj5', 'truth_ptj5', 'ptj6', 'truth_ptj6',\n",
    "    'etaj1', 'truth_etaj1', 'etaj2', 'truth_etaj2',\n",
    "    'etaj3', 'truth_etaj3', 'etaj4', 'truth_etaj4',\n",
    "    'etaj5', 'truth_etaj5', 'etaj6', 'truth_etaj6',\n",
    "    'phij1', 'truth_phij1', 'phij2', 'truth_phij2',\n",
    "    'phij3', 'truth_phij3', 'phij4', 'truth_phij4',\n",
    "    'phij5', 'truth_phij5', 'phij6', 'truth_phij6',\n",
    "    'mj1', 'truth_mj1', 'mj2', 'truth_mj2',\n",
    "    'mj3', 'truth_mj3', 'mj4', 'truth_mj4',\n",
    "    'mj5', 'truth_mj5', 'mj6', 'truth_mj6',\n",
    "    'met', 'truth_met', 'metphi', 'truth_metphi',\n",
    "    'ptwhad', 'truth_ptwhad', 'truth_pt_particlewhad',\n",
    "    'rapiditywhad', 'truth_rapidity_particlewhad',\n",
    "    'mwhad', 'truth_mwhad',\n",
    "    'mbl_selected', 'truth_m_particlebl',\n",
    "    'mbwhad_selected', 'truth_m_particlebwhad',\n",
    "    'dRbl_selected', 'truth_dR_particlebl',\n",
    "    'dRbwhad_selected', 'truth_dR_particlebwhad',\n",
    "    'pass_particle', 'pass_reco'\n",
    "    ]\n",
    "\n",
    "    # Fill missing fields with None and keep only listed fields in specified order\n",
    "    def harmonize_array(array, target_fields):\n",
    "        for field in target_fields:\n",
    "            if field not in array.fields:\n",
    "                array = ak.with_field(array, None, field)\n",
    "        return ak.zip({field: array[field] for field in target_fields})\n",
    "    print(\"Start harmonizing fields\")\n",
    "    # Apply harmonization\n",
    "    truth_array = harmonize_array(particle_only, reco_field_order)\n",
    "    cut_array = harmonize_array(cut_array, reco_field_order)\n",
    "    print(\"Done harmonizing\")\n",
    "    df_particle = ak.to_dataframe(particle_only)\n",
    "    df_reco = ak.to_dataframe(reco_only)\n",
    "    df_matched = ak.to_dataframe(cut_array)\n",
    "    print(\"Converted to pandas - Start concatenating\")\n",
    "    # Concatenate\n",
    "    df_final = pd.concat([df_particle, df_reco, df_matched], ignore_index=True)\n",
    "\n",
    "    #final_array = ak.concatenate([particle_only, reco_only, cut_array], axis=0)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ee6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_double_counting(array):\n",
    "    ids = array[\"event\"]\n",
    "\n",
    "    # Use numpy to find unique ones and return indices of the first occurrences\n",
    "    _, unique_index = np.unique(ak.to_numpy(ids), return_index=True)\n",
    "\n",
    "    # Sort indices to preserve original order\n",
    "    unique_index = np.sort(unique_index)\n",
    "\n",
    "    # Select only unique entries\n",
    "    unique_awkward = array[unique_index]\n",
    "    return unique_awkward\n",
    "\n",
    "#truth_array = avoid_double_counting(truth_array)\n",
    "#herwig_array = avoid_double_counting(herwig_array)\n",
    "#pythia_array_reco_cuts = avoid_double_counting(pythia_array_reco_cuts)\n",
    "#pythia_array_matched_cuts = avoid_double_counting(pythia_array_matched_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1482ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_pass_particle(df):\n",
    "    \"\"\"\n",
    "    Ensure consistency: pass_reco can only be True if key reco observables are non-zero.\n",
    "    Key reco observables: ptl1, ptj1, ptb1\n",
    "    \"\"\"\n",
    "    key_reco = ['truth_ptj1', 'truth_ptj2']\n",
    "    \n",
    "    # Create a mask: pass_reco should be True only if all key_reco > 0\n",
    "    mask_valid = (df[key_reco] != 0.0).all(axis=1)\n",
    "    \n",
    "    # Set pass_reco to False where key observables are zero\n",
    "    df.loc[df['pass_particle'] & ~mask_valid, 'pass_particle'] = False\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec457166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reco_herwig = '/scratch/mjosef/OMNIFOLD_Tutorial/datasets/WbWb_files/bulk_region/ttbar_enhanced_Herwig71_fast.Nominal.root'\n",
    "#truth_pythia = '/scratch/mjosef/OMNIFOLD_Tutorial/datasets/WbWb_files/bulk_region/truth_ttbar_enhanced.Nominal.root'\n",
    "#reco_pythia = '/scratch/mjosef/OMNIFOLD_Tutorial/datasets/WbWb_files/bulk_region/ttbar_enhanced.Nominal.root'\n",
    "#\n",
    "#\n",
    "#herwig_file = uproot.open(reco_herwig)\n",
    "#herwig_tree = herwig_file['NOSYS/4j_incl']\n",
    "#\n",
    "#pythia_truth_file = uproot.open(truth_pythia)\n",
    "#pythia_truth_tree = pythia_truth_file['NOSYS/physics']\n",
    "#\n",
    "#pythia_reco_file = uproot.open(reco_pythia)\n",
    "#pythia_reco_tree = pythia_reco_file['NOSYS/4j_incl']\n",
    "#pythia_matched_tree = pythia_reco_file['NOSYS/matched_4j_incl']\n",
    "\n",
    "#herwig_array = herwig_tree.arrays()\n",
    "#pythia_array = pythia_tree.arrays()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# --- USER CONFIGURATION ---\n",
    "# --------------------------------------------------------\n",
    "weights_folder = '/scratch/mjosef/Unfolding/omnifold/weights_real-data'\n",
    "omnifold_name  = \"WWbb_transformer_real-data\"   # whatever prefix you used\n",
    "n_iterations   = 4\n",
    "n_ensembles    = 1                  # or 3 if you used ensemble\n",
    "\n",
    "\n",
    "# luminosity & normalization\n",
    "luminosity = 3244.54 + 33402.2 + 44630.6 + 58791.6\n",
    "SF = (1 / luminosity) * (1 / train_fraction)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "def load_unfolded_weights(iteration, stepn, ensemble=0):\n",
    "    \"\"\"\n",
    "    Load OmniFold weights for a given iteration and step.\n",
    "    \"\"\"\n",
    "    model_name = f\"{weights_folder}/OmniFold_{name}_iter{iteration}_step{stepn}\"\n",
    "    if n_ensembles > 1:\n",
    "        model_name += f\"_ensemble{ensemble}\"\n",
    "    model_name += \".weights.h5\"\n",
    "\n",
    "    if not os.path.exists(model_name):\n",
    "        print(f\"âŒ Missing: {model_name}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"âœ… Loaded weights: {model_name}\")\n",
    "\n",
    "    # Example: load reweighted model and evaluate on generator-level MC\n",
    "    my_model = omnifold.model2\n",
    "    my_model.load_weights(model_name)\n",
    "    unfolded_weights = my_model.predict(pythia_truth_test, batch_size=1000).flatten()\n",
    "\n",
    "    return unfolded_weights\n",
    "\n",
    "\n",
    "def plot_iteration(iteration, unfolded_weights):\n",
    "    \"\"\"\n",
    "    Reproduce your TUnfold comparison plot for a given iteration.\n",
    "    \"\"\"\n",
    "    hist = TUnfold_incl_file['unfolding_ptl1_NOSYS']\n",
    "    rel_ptl1_up = TUnfold_incl_file['unfolding_error_ptl1_direct_STAT_DATA__1up;1']\n",
    "    rel_ptl1_down = TUnfold_incl_file['unfolding_error_ptl1_direct_STAT_DATA__1down;1']\n",
    "\n",
    "    values = hist.values()\n",
    "    edges = hist.axis().edges()\n",
    "\n",
    "    bin_widths = np.diff(edges)\n",
    "    rel_unc_up_tunfold = rel_ptl1_up.values()\n",
    "    rel_unc_down_tunfold = rel_ptl1_down.values()\n",
    "\n",
    "    # --- OmniFold histograms ---\n",
    "    weights_omnifold = (unfolded_weights * pythia_weights_test)[pythia_pass_gen_test & flags_mc] * SF\n",
    "    weights_omnifold2 = weights_omnifold**2\n",
    "    values_omnifold = pythia_truth_test[:, 0, 0][pythia_pass_gen_test & flags_mc]\n",
    "\n",
    "    counts2, _ = np.histogram(values_omnifold, bins=edges, weights=weights_omnifold)\n",
    "    counts2_density = counts2 / bin_widths\n",
    "\n",
    "    sum_w2, _ = np.histogram(values_omnifold, bins=edges, weights=weights_omnifold2)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        rel_unc_omnifold = np.sqrt(sum_w2) / counts2\n",
    "        rel_unc_omnifold[~np.isfinite(rel_unc_omnifold)] = 0\n",
    "\n",
    "    # --- Pythia reference ---\n",
    "    counts1, _, _ = plt.hist(\n",
    "        pythia_loader.gen[:, 0, 0][pythia_loader.pass_gen],\n",
    "        weights=pythia_loader.weight[pythia_loader.pass_gen] * SF,\n",
    "        bins=edges,\n",
    "        histtype='step'\n",
    "    )\n",
    "    plt.clf()  # clear dummy hist\n",
    "\n",
    "    counts1_density = counts1 / bin_widths\n",
    "    values_density = values / bin_widths\n",
    "    abs_unc_up_tunfold = rel_unc_up_tunfold * values_density\n",
    "    abs_unc_down_tunfold = rel_unc_down_tunfold * values_density\n",
    "    bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "    # --- Figure ---\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8),\n",
    "                                   gridspec_kw={'height_ratios': [3, 1]},\n",
    "                                   sharex=True)\n",
    "\n",
    "    ax1.step(edges[:-1], counts1_density, where=\"post\", label=r\"Pythia8 $t\\bar{t}$\")\n",
    "    ax1.step(edges[:-1], counts2_density, where=\"post\", label=f\"OmniFold Iter {iteration+1}\")\n",
    "    yerr = np.vstack((abs_unc_down_tunfold, abs_unc_up_tunfold))\n",
    "    ax1.errorbar(bin_centers[:-1], values_density[:-1], yerr=yerr[:, :-1],\n",
    "                 fmt='o', color='black', capsize=3, markersize=4, label=\"TUnfold (stat. unc.)\")\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylabel(\"cross-section [pb/GeV]\")\n",
    "    ax1.legend()\n",
    "    ax1.set_title(f\"OmniFold vs TUnfold (Iteration {iteration+1})\")\n",
    "\n",
    "    # --- Ratio ---\n",
    "    ratio2 = np.divide(counts2_density, values_density, out=np.zeros_like(counts2_density), where=values_density != 0)\n",
    "    ratio3 = np.ones_like(values_density)\n",
    "    upper_omnifold = ratio2 * (1 + rel_unc_omnifold)\n",
    "    lower_omnifold = ratio2 * (1 - rel_unc_omnifold)\n",
    "    err_up_tunfold = ratio3 * rel_ptl1_up.values()\n",
    "    err_down_tunfold = ratio3 * rel_ptl1_down.values()\n",
    "    yerr_tunfold = np.vstack((err_down_tunfold, err_up_tunfold))\n",
    "\n",
    "    ax2.step(edges[:-1], ratio2, where=\"post\", color=\"orange\", label=\"Omnifold / TUnfold\")\n",
    "    ax2.errorbar(bin_centers[:-1], ratio3[:-1], yerr=yerr_tunfold[:, :-1],\n",
    "                 fmt='o', color='black', capsize=3, label='TUnfold stat. unc.')\n",
    "    ax2.fill_between(edges[:-1], lower_omnifold, upper_omnifold, step='post', alpha=0.2,\n",
    "                     color='orange', label='Omnifold stat. unc.')\n",
    "    ax2.set_xlabel(\"Lepton-pT [GeV]\")\n",
    "    ax2.set_ylabel(\"Ratio to TUnfold\")\n",
    "    ax2.set_xlim(30, 700)\n",
    "    ax2.set_ylim(0.5, 1.5)\n",
    "    ax2.legend(fontsize=8)\n",
    "    ax2.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{weights_folder}/plot_iter{iteration+1}.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ“ˆ Saved plot for iteration {iteration+1}\")\n",
    "    \n",
    "\n",
    "# --------------------------------------------------------\n",
    "# MAIN LOOP\n",
    "# --------------------------------------------------------\n",
    "for i in range(n_iterations):\n",
    "    unfolded_weights = load_unfolded_weights(i, stepn=2)  # use step2 model output\n",
    "    if unfolded_weights is not None:\n",
    "        plot_iteration(i, unfolded_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_data_shaper(df):\n",
    "\n",
    "    particle_names = ['l1', 'b1', 'b2', 'b3', 'b4', 'j1', 'j2', 'j3', 'j4', 'j5', 'j6']\n",
    "\n",
    "    # Build reco and truth arrays\n",
    "    reco_pts  = [df[f'pt{p}']   for p in particle_names]\n",
    "    reco_etas = [df[f'eta{p}']  for p in particle_names]\n",
    "    reco_phis = [df[f'phi{p}']  for p in particle_names]\n",
    "    \n",
    "    # Masses: lepton = 0, b-jets = mb1â€“4, jets = mj1â€“6\n",
    "    reco_masses = [np.zeros_like(df['ptl1'])] + \\\n",
    "                  [df[f'mb{i}'] for i in range(1, 5)] + \\\n",
    "                  [df[f'mj{i}'] for i in range(1, 7)]\n",
    "    # Stack into (12 particles, n_events, 4)\n",
    "    reco_features = np.stack([reco_pts, reco_etas, reco_phis, reco_masses], axis=-1)\n",
    "    reco_features = reco_features.transpose(1, 0, 2)  # â†’ (n_events, 12, 4)\n",
    "\n",
    "    # Neutrino (reco): met, eta=0, phi=metphi, mass=0\n",
    "    met_pt = df['met']\n",
    "    met_phi = df['metphi']\n",
    "    zeros = np.zeros_like(met_pt)\n",
    "    neutrino_reco = np.stack([met_pt, zeros, met_phi, zeros], axis=-1)  # (n_events, 4)\n",
    "\n",
    "    # Append neutrino\n",
    "    reco_features = np.concatenate([reco_features, neutrino_reco[:, None, :]], axis=1)  # (n_events, 12, 4)\n",
    "    truth_pts  = [df[f'truth_pt{p}']   for p in particle_names]\n",
    "    truth_etas = [df[f'truth_eta{p}']  for p in particle_names]\n",
    "    truth_phis = [df[f'truth_phi{p}']  for p in particle_names]\n",
    "    \n",
    "    truth_masses = [np.zeros_like(df['truth_ptl1'])] + \\\n",
    "                   [df[f'truth_mb{i}'] for i in range(1, 5)] + \\\n",
    "                   [df[f'truth_mj{i}'] for i in range(1, 7)]\n",
    "\n",
    "    truth_features = np.stack([truth_pts, truth_etas, truth_phis, truth_masses], axis=-1)\n",
    "    truth_features = truth_features.transpose(1, 0, 2)\n",
    "\n",
    "    # Neutrino (truth): use 'truth_met' and 'truth_met_phi'\n",
    "    truth_met_pt = df['truth_met']\n",
    "    truth_met_phi = df['truth_metphi']\n",
    "    zeros_truth = np.zeros_like(truth_met_pt)\n",
    "    neutrino_truth = np.stack([truth_met_pt, zeros_truth, truth_met_phi, zeros_truth], axis=-1)\n",
    "\n",
    "    truth_features = np.concatenate([truth_features, neutrino_truth[:, None, :]], axis=1)\n",
    "    \n",
    "    return reco_features, truth_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pass_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds 'pass_reco' and 'pass_particle' columns based on 'pass_matched'.\n",
    "    \n",
    "    Rules:\n",
    "    - pass_matched == 0 -> pass_reco = 1, pass_particle = 0\n",
    "    - pass_matched == 2 -> pass_reco = 0, pass_particle = 1\n",
    "    - pass_matched == 1 -> pass_reco = 1, pass_particle = 1\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df[\"pass_reco\"] = 0\n",
    "    df[\"pass_particle\"] = 0\n",
    "\n",
    "    # Apply the rules\n",
    "    df.loc[df[\"pass_matched\"] == 0, \"pass_reco\"] = 1\n",
    "    df.loc[df[\"pass_matched\"] == 0, \"pass_particle\"] = 0\n",
    "\n",
    "    df.loc[df[\"pass_matched\"] == 2, \"pass_reco\"] = 0\n",
    "    df.loc[df[\"pass_matched\"] == 2, \"pass_particle\"] = 1\n",
    "\n",
    "    df.loc[df[\"pass_matched\"] == 1, \"pass_reco\"] = 1\n",
    "    df.loc[df[\"pass_matched\"] == 1, \"pass_particle\"] = 1\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Apptainer)",
   "language": "python",
   "name": "apptainer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
